{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import librosa.display\n",
    "import IPython.display as ipd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial.distance import pdist, cdist\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import soundfile as sf\n",
    "from prettytable import PrettyTable\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.signal import hilbert, butter, filtfilt, medfilt, lfilter, wiener, convolve"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DICCIONARIOS CON LAS RUTAS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fruit_types = ['pera', 'banana', 'manzana', 'naranja']\n",
    "audios = {fruit: [] for fruit in fruit_types}\n",
    "root_dir = '../../dataset'\n",
    "\n",
    "for dirname, _, filenames in os.walk(root_dir):\n",
    "    fruit_type = os.path.basename(dirname)\n",
    "    if fruit_type in fruit_types:\n",
    "        audios[fruit_type].extend([os.path.join(dirname, filename) for filename in filenames if filename.endswith('.wav')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed = {fruit: [] for fruit in fruit_types}\n",
    "\n",
    "for dirname, _, filenames in os.walk(root_dir):\n",
    "    path = os.path.basename(dirname)\n",
    "    if path == 'processed':\n",
    "        fruit_type = os.path.basename(os.path.dirname(dirname))\n",
    "        if fruit_type in fruit_types:\n",
    "            processed[fruit_type].extend([os.path.join(dirname, filename) for filename in filenames if filename.endswith('.wav')])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CARACTERISTICAS DEL AUDIO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "FRAME_SIZE = 2048 # In the documentation says it's convenient for speech.C\n",
    "HOP_SIZE   = int(FRAME_SIZE/2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FUNCIONES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio(audiofile):\n",
    "    test_audio, sr = librosa.load(audiofile, sr = None)\n",
    "    duration = librosa.get_duration(filename=audiofile, sr=sr)\n",
    "    return test_audio, sr, duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_vector(signal, duration):\n",
    "    return np.linspace(0, duration, len(signal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rms(signal):\n",
    "    return librosa.feature.rms(y=signal, frame_length = FRAME_SIZE, hop_length = HOP_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative(signal, duration):\n",
    "    signal = signal.reshape(-1,)\n",
    "    dy = np.gradient(signal, np.linspace(0, duration, len(signal)))\n",
    "    return dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(signal):\n",
    "    peak = np.max(signal)\n",
    "    signal/=peak\n",
    "    return signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean(audio_in):\n",
    "    signal, sr, _ = load_audio(audio_in)\n",
    "    frames = librosa.util.frame(signal, frame_length=512, hop_length=256)\n",
    "    mean= np.mean(frames, axis=0)\n",
    "\n",
    "# Crear un arreglo de tiempo para la visualización\n",
    "    tiempo = librosa.times_like(mean, sr=sr, hop_length=256)\n",
    "    \n",
    "# Visualizar los valores medios en el tiempo\n",
    "    plt.figure(figsize=(15, 4))\n",
    "    #librosa.display.waveshow(signal, sr=sr, alpha=0.5)\n",
    "    plt.plot(tiempo, mean, color='r', linewidth=2)\n",
    "    plt.title('Señal de Audio y Valor Medio por Frame')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def med_filter(audio_in, audio_out, window = 3):\n",
    "    signal, sr, _ = load_audio(audio_in)\n",
    "    filtered = medfilt(signal, kernel_size=window)\n",
    "    sf.write(audio_out, filtered, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def low_pass_filter(audio_in, audio_out, cutoff_frequency = 5000):\n",
    "    signal, sr, _ = load_audio(audio_in)\n",
    "    nyquist = 0.5 * sr\n",
    "    cutoff = cutoff_frequency / nyquist\n",
    "    b, a = butter(N=6, Wn=cutoff, btype='low', analog=False, output='ba')\n",
    "    \n",
    "    filtered = lfilter(b, a, signal)\n",
    "    sf.write(audio_out, filtered, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_filter(audio_in, audio_out, alpha = 0.1):\n",
    "    signal, sr, _ = load_audio(audio_in)\n",
    "    filtered  = lfilter([1 - alpha], [1, -alpha], signal)\n",
    "    sf.write(audio_out, filtered, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wiener_filter(audio_in, audio_out, noise = 0.9):\n",
    "    signal, sr, _ = load_audio(audio_in)\n",
    "    filtered = wiener(signal, noise = noise)\n",
    "    sf.write(audio_out, filtered, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_movil_filter(audio_in, audio_out, ventana=3):\n",
    "    # Definir el kernel de promedio móvil\n",
    "    kernel = np.ones(ventana) / ventana\n",
    "\n",
    "    signal, sr, _ = load_audio(audio_in)\n",
    "    # Aplicar el filtro de promedio móvil\n",
    "    filtered = convolve(signal, kernel, mode='same')\n",
    "    \n",
    "    sf.write(audio_out, filtered, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_envelope(audio, cutoff_frequency = 10.0):\n",
    "    signal, sr, duration = load_audio(audio)\n",
    "    analytic_signal = hilbert(signal)\n",
    "\n",
    "    # Calcular la envolvente de amplitud\n",
    "    envelope = np.abs(analytic_signal)\n",
    "\n",
    "    # Aplicar filtro pasa bajos para suavizar la envolvente\n",
    "    nyquist = 0.5 * sr\n",
    "    cutoff = cutoff_frequency / nyquist\n",
    "    b, a = butter(N=6, Wn=cutoff, btype='low', analog=False, output='ba')\n",
    "    \n",
    "    smoothed = filtfilt(b, a, envelope)\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    # Visualizar la envolvente de amplitud\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(np.arange(len(envelope)) / sr, envelope, alpha=0.5, label='Envolvente Original')\n",
    "    plt.title('Envolvente de Amplitud Original')\n",
    "    plt.xlabel('Tiempo (s)')\n",
    "    plt.ylabel('Amplitud')\n",
    "    plt.legend()\n",
    "\n",
    "    # Visualizar la envolvente suavizada\n",
    "    print(len(smoothed))\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(np.arange(len(smoothed)) / sr, smoothed, label='Envolvente Suavizada', color='orange')\n",
    "    plt.title('Envolvente de Amplitud Suavizada')\n",
    "    plt.xlabel('Tiempo (s)')\n",
    "    plt.ylabel('Amplitud')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preemphasis(audio_in, coef=0.97):\n",
    "    signal, _, _ = load_audio(audio_in)\n",
    "    return np.append(signal[0], signal[1:] - coef * signal[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim(audio_in, audio_out, umbral = 0.35):\n",
    "    signal, sr, duration = load_audio(audio_in)\n",
    "    filtered = preemphasis(audio_in, 0.975)\n",
    "    rms_signal = rms(filtered)\n",
    "\n",
    "    rms_signal = normalize(rms_signal)\n",
    "    drms = normalize(derivative(rms_signal, duration))\n",
    "\n",
    "    audio_vector = time_vector(signal, duration)\n",
    "    drms_vector = time_vector(drms, duration)\n",
    "\n",
    "    left_index = np.argmax(np.abs(drms) > umbral) - 1\n",
    "    rigth_index = len(drms) - 1 - np.argmax(np.abs(np.flip(drms)) > umbral) + 1\n",
    "\n",
    "    left_time = drms_vector[left_index]\n",
    "    rigth_time = drms_vector[rigth_index]\n",
    "\n",
    "    mask_vector = audio_vector >= left_time\n",
    "\n",
    "    audio_vector = audio_vector[mask_vector]\n",
    "    trimed_signal = signal[mask_vector]\n",
    "\n",
    "    mask_vector = audio_vector <= rigth_time\n",
    "\n",
    "    audio_vector = audio_vector[mask_vector]\n",
    "    trimed_signal = trimed_signal[mask_vector]\n",
    "\n",
    "    sf.write(audio_out, trimed_signal, sr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FUNCIONES DE EFECTIVIDAD**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sphere(vectors):\n",
    "    center = np.mean(vectors, axis = 0)\n",
    "    center = center.reshape(1, -1)\n",
    "    radius = cdist(center, vectors).max()     # Pairwise distance\n",
    "    return radius, center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_centers(features):\n",
    "    centers = dict.fromkeys(features.keys())\n",
    "    for fruit, group in features.items():\n",
    "        _, center = get_sphere(group)\n",
    "        centers[fruit] = center\n",
    "    return centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_radiuses(features):\n",
    "    radiuses = dict.fromkeys(features.keys())\n",
    "    for fruit, group in features.items():\n",
    "        radius, _ = get_sphere(group)\n",
    "        radiuses[fruit] = radius\n",
    "    return radiuses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_overlaps(fruit_features):\n",
    "    centers = get_centers(fruit_features)\n",
    "    radiuses = get_radiuses(fruit_features)\n",
    "    overlaps = dict.fromkeys(fruit_features.keys())\n",
    "    \n",
    "    # A dictionary of dictionarys. Keys, the fruit types\n",
    "    for key in overlaps:\n",
    "        # Each dictionary in the dictionary\n",
    "        overlaps[key] = dict.fromkeys(fruit_types)\n",
    "    \n",
    "    for i in range(len(fruit_types)):\n",
    "        for j in range(i + 1, len(fruit_types)):\n",
    "            distancesAB = cdist(centers[fruit_types[i]], fruit_features[fruit_types[j]])\n",
    "            distancesBA = cdist(centers[fruit_types[j]], fruit_features[fruit_types[i]])\n",
    "\n",
    "            mask_distancesAB = distancesAB < radiuses[fruit_types[i]]\n",
    "            mask_distancesBA = distancesBA < radiuses[fruit_types[j]]\n",
    "\n",
    "            numberBinA = np.count_nonzero(mask_distancesAB)\n",
    "            numberAinB = np.count_nonzero(mask_distancesBA)\n",
    "\n",
    "            overlaps[fruit_types[i]][fruit_types[j]] = numberBinA\n",
    "            overlaps[fruit_types[j]][fruit_types[i]] = numberAinB\n",
    "    return overlaps # Each element is the number of vectors of one group in the sphere of another"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_components(centers, nc):\n",
    "    pacum = np.zeros((1, centers[fruit_types[0]].shape[1]))\n",
    "    pair_components = dict()\n",
    "    for i in range(len(fruit_types)):\n",
    "        for j in range(i + 1, len(fruit_types)):\n",
    "            dif = centers[fruit_types[i]] - centers[fruit_types[j]]\n",
    "            dist = cdist(centers[fruit_types[i]], centers[fruit_types[j]])\n",
    "            difp = (dif**2)*100/(dist**2)\n",
    "            pair_components[f\"{fruit_types[i]}-{fruit_types[j]}\"]= np.argsort(difp[0])[-nc:]\n",
    "            pacum += difp\n",
    "    index_max = np.argsort(pacum[0])[-nc:]\n",
    "    #return np.sort(index_max)\n",
    "    return index_max, pair_components"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PRINCIPAL**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**KNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn(training, test, k_n):\n",
    "    X = np.concatenate([v for v in training.values()], axis=0)\n",
    "    y = np.concatenate([[k] * v.shape[0] for k, v in training.items()])\n",
    "\n",
    "    #X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Crear clasificador KNN\n",
    "    knn_classifier = KNeighborsClassifier(n_neighbors = k_n)\n",
    "\n",
    "    # Entrenar el clasificador\n",
    "    knn_classifier.fit(X, y)\n",
    "\n",
    "    # Predecir las etiquetas para los datos de prueba\n",
    "    predicted_fruit = knn_classifier.predict(test)\n",
    "\n",
    "    print(f'La fruta predicha para el nuevo audio es: {predicted_fruit[0]}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PLOTEO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2d\n",
    "fig = plt.figure()\n",
    "\n",
    "colors = dict(zip(fruit_types,['green','yellow','red','orange']))\n",
    "center_colors  = dict(zip(fruit_types,['blue','brown','black','cyan']))\n",
    "\n",
    "for fruit, points in features.items():\n",
    "    plt.scatter(points[:, 0], points[:, 1], c = colors[fruit], label=fruit)\n",
    "\n",
    "plt.xlabel('Eje X')\n",
    "plt.ylabel('Eje Y')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3d\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "for fruit, points in features.items():\n",
    "    ax.scatter(points[:, 0], points[:, 1], points[:, 2], c=colors[fruit], marker='o', label=fruit)\n",
    "    #ax.scatter(centers[fruit][:, 0], centers[fruit][:, 1], centers[fruit][:, 2], c=center_colors[fruit], marker='o', label=f\"{fruit}-center\")\n",
    "\n",
    "# configure labels\n",
    "ax.set_xlabel('Eje X')\n",
    "ax.set_ylabel('Eje Y')\n",
    "ax.set_zlabel('Eje Z')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_signal(signal, duration, name):\n",
    "    time_vector = np.linspace(0, duration, len(signal))\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    #plot\n",
    "    #librosa.display.waveshow(signal)\n",
    "    \n",
    "    plt.plot(time_vector, signal, linestyle='-')\n",
    "    \n",
    "    # extra\n",
    "    plt.title(f'Señal de Audio {name} - Duración: {duration:.2f} segundos')\n",
    "    plt.xlabel('Tiempo (s)')\n",
    "    plt.ylabel('Amplitud')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#signal\n",
    "def plot_audio(audio):\n",
    "    signal, sr, duration = load_audio(audio)\n",
    "    plot_signal(signal, duration, os.path.basename(audio))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lo primero vamos a ver como funciona la función de recorte y los filtros**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trim de los audios\n",
    "for type in audios:\n",
    "    for i, _ in enumerate(audios[type]):\n",
    "        wiener_filter(audios[type][i], processed[type][i], 10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hemos visto que los filtros no funcionan muy bien.\n",
    "Al oido puede que algunos se escuche que funcionan bien, pero no hacen en realidad mucho por el ruido que queremos eliminar de unos audios, que es el ruido impulsivo al inicio y al final del audio o ruido constante al principio y al final del audio. El que quiesieramos aminorar o eliminar para luego hacer un recorte limpio del audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#comparacion gráficas\n",
    "for type in audios:\n",
    "    for i, _ in enumerate(audios[type]):\n",
    "        plot_signal(audios[type][i])\n",
    "        plot_signal(processed[type][i])\n",
    "        \n",
    "        input()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "problemas de ruido en pera6, banana6, banana7(un pico al final), banana8(ruido al inicio), manzana6(ruido al inicio), manzana7 (ruido al inicio), manzana8(ruido al final), naranja8(picos al final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fruit, group in audios.items():\n",
    "    for audio in group:\n",
    "        rms_signal, duration = rms(audio)\n",
    "\n",
    "        rms_signal = normalize(rms_signal)\n",
    "        drms = normalize(derivative(rms_signal, duration))\n",
    "        \n",
    "        plot_audio(audio)\n",
    "        \n",
    "        plot_signal(rms_signal.reshape(-1,1), duration, \"rms\")\n",
    "        plot_signal(drms, duration, 'drms/dt')\n",
    "        input()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probamos que es lo que pasa con la media de la señal de audio calculada por frames.\n",
    "Como que queda medio feo con la función del librosas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = audios['naranja'][6]\n",
    "rms_signal, duration = rms(audio)\n",
    "\n",
    "rms_signal = normalize(rms_signal)\n",
    "drms = normalize(derivative(rms_signal, duration))\n",
    "\n",
    "plot_audio(audio)\n",
    "plot_signal(rms_signal.reshape(-1,1), duration, \"rms\")\n",
    "plot_signal(drms, duration, 'drms/dt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probamos corte con la envolvente entonces.\n",
    "Bueno, me di cuenta que también se podría llegar a hacer directamente con el rms el recorte porque da mas o menos la misma forma que da la envolvente sin tanta configuración. Con la RMS pareciera que hay unna buena descripción de la envolvente de la seña y además permite el recorte del ruido por identificación de un cambio importante en la derivada de la rms de la señal. El punto de corte se puede identificar cuando la derivada de la señal de rms deja de ser constante y pasa a tener un cambio importante. 44 es lo que le pusimos a la frecuencia de corte y anduvo copado. Osea dependia enrealidad del hopzise que quedo la mitad de un fram size de 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = audios['pera'][6]\n",
    "plot_audio(audio)\n",
    "smooth_envelope(audio, cutoff_frequency = 44)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PROBAMOS EL FILTRO DE PREEMPHASIS\n",
    "El filtro de prehempasis anda rejoya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = audios['naranja'][6]\n",
    "signal, sr, duration = load_audio(audio)\n",
    "filtered = preemphasis(signal, 0.975)\n",
    "#sf.write(processed['naranja'][6], filtered, sr)\n",
    "plot_signal(normalize(signal), duration, os.path.basename(audio))\n",
    "plot_signal(normalize(filtered), duration, 'filtered')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora hay que probar el filtro de prehempasis con la derivada para tratar el corte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indice = 8\n",
    "fruit = 'naranja'\n",
    "umbral = 0.35\n",
    "\n",
    "audio_in = audios[fruit][indice]\n",
    "audio_out = processed[fruit][indice]\n",
    "signal,sr,_ = load_audio(audio_in)\n",
    "\n",
    "audio_vector = time_vector(audio_in)\n",
    "\n",
    "\n",
    "filtered = preemphasis(audio_in, audio_out, 0.975)\n",
    "rms_signal, duration = rms(audio_out)\n",
    "\n",
    "rms_signal = normalize(rms_signal)\n",
    "drms = normalize(derivative(rms_signal, duration))\n",
    "\n",
    "drms_vector = np.linspace(0, duration, len(drms))\n",
    "\n",
    "\n",
    "left_index = np.argmax(np.abs(drms) > umbral) - 1\n",
    "rigth_index = len(drms) - 1 - np.argmax(np.abs(np.flip(drms)) > umbral) + 1\n",
    "\n",
    "left_time = drms_vector[left_index]\n",
    "rigth_time = drms_vector[rigth_index]\n",
    "\n",
    "\n",
    "mask_vector = audio_vector >= left_time\n",
    "audio_vector = audio_vector[mask_vector]\n",
    "\n",
    "trimed_signal = signal[mask_vector]\n",
    "\n",
    "mask_vector = audio_vector <= rigth_time\n",
    "audio_vector = audio_vector[mask_vector]\n",
    "trimed_signal = trimed_signal[mask_vector]\n",
    "\n",
    "sf.write(audio_out,trimed_signal,sr)\n",
    "# Seleccionar la porción de la señal entre los índices encontrados\n",
    "#trimed = drms[left_index:rigth_index + 1]\n",
    "\n",
    "\n",
    "plot_audio(audio_in)\n",
    "#plot_signal(rms_signal.reshape(-1,1), duration, \"rms\")\n",
    "#plot_signal(drms, duration, 'drms/dt')\n",
    "#plot_signal(trimed, left_time-rigth_time, 'trimed')\n",
    "plot_signal(trimed_signal, left_time-rigth_time, f\"trimed - {os.path.basename(audio_in)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PRUEBA DE LA APLICACIÓN DEL CORTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Juan\\AppData\\Local\\Temp\\ipykernel_916\\1587896153.py:3: FutureWarning: get_duration() keyword argument 'filename' has been renamed to 'path' in version 0.10.0.\n",
      "\tThis alias will be removed in version 1.0.\n",
      "  duration = librosa.get_duration(filename=audiofile, sr=sr)\n"
     ]
    }
   ],
   "source": [
    "#procesamiento del audio\n",
    "for fruit, group in audios.items():\n",
    "    for i, _ in enumerate(group):\n",
    "        trim(audios[fruit][i], processed[fruit][i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
