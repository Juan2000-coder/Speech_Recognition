{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.signal import hilbert, butter, lfilter\n",
    "import soundfile as sf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RUTAS Y TIPOS DE FRUTAS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "fruit_types      = ['pera', 'banana', 'manzana', 'naranja']\n",
    "audios           = {fruit: [] for fruit in fruit_types}\n",
    "dataset_path     = '../../dataset'\n",
    "original_path    = os.path.join(dataset_path, 'original')\n",
    "processed_path   = os.path.join(dataset_path, 'processed')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DICCIONARIO DE AUDIOS ORIGINALES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "original = {fruit: [] for fruit in fruit_types}\n",
    "for dirname, _, filenames in os.walk(original_path):\n",
    "    subdir = os.path.basename(dirname)\n",
    "    if subdir in fruit_types:\n",
    "        original[subdir].extend([os.path.join(dirname, filename) for filename in filenames if filename.endswith('.wav')])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DICCIONARIO DE AUDIOS PROCESADOS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed = {fruit: [] for fruit in fruit_types}\n",
    "for dirname, _, filenames in os.walk(processed_path):\n",
    "    subdir = os.path.basename(dirname)\n",
    "    if subdir in fruit_types:\n",
    "        processed[subdir].extend([os.path.join(dirname, filename) for filename in filenames if filename.endswith('.wav')])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PARAMETROS DEL AUDIO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "FRAME_SIZE = 1024# In the documentation says it's convenient for speech.C\n",
    "HOP_SIZE   = int(FRAME_SIZE/2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FUNCIONES GENERALES DE AUDIO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio(audiofile):\n",
    "    test_audio, sr = librosa.load(audiofile, sr = None)\n",
    "    duration = librosa.get_duration(filename=audiofile, sr=sr)\n",
    "    return test_audio, sr, duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_vector(signal, duration):\n",
    "    return np.linspace(0, duration, len(signal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rms(signal, frames, hop):\n",
    "    return librosa.feature.rms(y=signal, frame_length = frames, hop_length = hop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(signal):\n",
    "    peak = np.max(signal)\n",
    "    signal/=peak\n",
    "    return signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivative(signal, duration):\n",
    "    signal = signal.reshape(-1,)\n",
    "    dy = np.gradient(signal, np.linspace(0, duration, len(signal)))\n",
    "    return dy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FILTERS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def low_pass_filter(signal, sr, cutoff_frequency = 5000):\n",
    "    nyquist = 0.5 * sr\n",
    "    cutoff = cutoff_frequency / nyquist\n",
    "    b, a = butter(N=6, Wn=cutoff, btype='low', analog=False, output='ba')\n",
    "    filtered = lfilter(b, a, signal)\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def band_pass_filter(signal, sr, low_cutoff, high_cutoff):\n",
    "    b, a = butter(N=3, Wn = [low_cutoff, high_cutoff], btype='band', fs=sr)\n",
    "    return lfilter(b, a, signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preemphasis(signal, coef=0.97):\n",
    "    return np.append(signal[0], signal[1:] - coef * signal[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def envelope(signal):\n",
    "    analytic_signal = hilbert(signal)\n",
    "    return np.abs(analytic_signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_envelope(signal, sr, cutoff_frequency=50.0):\n",
    "    return low_pass_filter(envelope(signal), sr, cutoff_frequency)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PRROCCESSING OF THE AUDIO FILES FUNCTIONS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(audio_in, audio_out, umbral = 0.295):\n",
    "    signal, sr, duration = load_audio(audio_in)\n",
    "    \n",
    "    filtered = low_pass_filter(signal, sr, 1800)\n",
    "    filtered = preemphasis(filtered, 0.999)\n",
    "\n",
    "    rms_signal = rms(signal, 4096, 2048)\n",
    "\n",
    "    rms_signal = normalize(rms_signal)\n",
    "    drms = normalize(derivative(rms_signal, duration))\n",
    "\n",
    "    audio_vector = time_vector(signal, duration)\n",
    "    drms_vector = time_vector(drms, duration)\n",
    "\n",
    "    left_index = np.argmax(np.abs(drms) > umbral)\n",
    "    rigth_index = len(drms) - 1 - np.argmax(np.abs(np.flip(drms)) > umbral)\n",
    "\n",
    "    left_time = drms_vector[left_index]\n",
    "    rigth_time = drms_vector[rigth_index]\n",
    "\n",
    "    mask_vector = audio_vector >= left_time\n",
    "\n",
    "    audio_vector = audio_vector[mask_vector]\n",
    "    trimed_signal = signal[mask_vector]\n",
    "\n",
    "    mask_vector = audio_vector <= rigth_time\n",
    "\n",
    "    audio_vector = audio_vector[mask_vector]\n",
    "    trimed_signal = trimed_signal[mask_vector]\n",
    "\n",
    "    sf.write(audio_out, trimed_signal, sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_audios(original:dict, processed:dict):\n",
    "    already_processed = []\n",
    "    for group in processed.values():\n",
    "        already_processed.extend(group)\n",
    "        \n",
    "    for fruit, audios in original.items():\n",
    "        for audio in audios:\n",
    "            file = os.path.basename(audio)\n",
    "            if file in already_processed:\n",
    "                pass\n",
    "            else:\n",
    "                process(audio, os.path.join(processed_path, f\"{fruit}/{file}\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PLOTTING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2d\n",
    "def plot_features2d(features):\n",
    "    fig = plt.figure()\n",
    "    colors = dict(zip(fruit_types,['green','yellow','red','orange']))\n",
    "    \n",
    "\n",
    "    for fruit, points in features.items():\n",
    "        plt.scatter(points[:, 0], points[:, 1], c = colors[fruit], label=fruit)\n",
    "\n",
    "    plt.xlabel('Eje X')\n",
    "    plt.ylabel('Eje Y')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3d\n",
    "def plot_features3d(features):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    colors = dict(zip(fruit_types,['green','yellow','red','orange']))\n",
    "\n",
    "    for fruit, points in features.items():\n",
    "        ax.scatter(points[:, 0], points[:, 1], points[:, 2], c=colors[fruit], marker='o', label=fruit)\n",
    "        \n",
    "    ax.set_xlabel('Eje X')\n",
    "    ax.set_ylabel('Eje Y')\n",
    "    ax.set_zlabel('Eje Z')\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AUDIO PROCESSING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Juan\\AppData\\Local\\Temp\\ipykernel_5372\\1587896153.py:3: FutureWarning: get_duration() keyword argument 'filename' has been renamed to 'path' in version 0.10.0.\n",
      "\tThis alias will be removed in version 1.0.\n",
      "  duration = librosa.get_duration(filename=audiofile, sr=sr)\n"
     ]
    }
   ],
   "source": [
    "process_audios(original, processed)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FEATURES EXTRACTION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Juan\\AppData\\Local\\Temp\\ipykernel_4804\\1587896153.py:3: FutureWarning: get_duration() keyword argument 'filename' has been renamed to 'path' in version 0.10.0.\n",
      "\tThis alias will be removed in version 1.0.\n",
      "  duration = librosa.get_duration(filename=audiofile, sr=sr)\n"
     ]
    }
   ],
   "source": [
    "# Features extraction\n",
    "features = dict.fromkeys(fruit_types)\n",
    "split_frequency = 3000\n",
    "cuton = 20\n",
    "cutoff = 8500\n",
    "n_mfcc = 4\n",
    "\n",
    "for fruit, audios in processed.items():\n",
    "    features[fruit] = None\n",
    "    \n",
    "    for audio in audios:\n",
    "        # Load the audio signal\n",
    "        signal, sr, duration = load_audio(audio)\n",
    "\n",
    "        # Empty row of features\n",
    "        feature = np.empty((1, 0))\n",
    "\n",
    "        # Calculate the rms\n",
    "        audio_rms = np.sqrt(np.mean(signal**2))/np.max(signal)\n",
    "        feat = audio_rms\n",
    "        feature = np.append(feature, audio_rms)\n",
    "\n",
    "        # BER min\n",
    "        spec = librosa.stft(signal, n_fft = FRAME_SIZE, hop_length = HOP_SIZE)\n",
    "        BER  = band_energy_ratio(spec, split_frequency, sr)\n",
    "        feat = np.min(BER)\n",
    "        #feature = np.append(feature, feat)\n",
    "\n",
    "        # Centroid\n",
    "        centroid = librosa.feature.spectral_centroid(y=signal, sr=sr, n_fft=FRAME_SIZE, hop_length=HOP_SIZE)[0]\n",
    "        centroid /= np.max(np.abs(centroid))\n",
    "        # Varnz\n",
    "        feat = np.var(centroid)\n",
    "        #feature = np.append(feature, feat)\n",
    "        # std\n",
    "        feat = np.std(centroid)/np.mean(centroid)\n",
    "        feature = np.append(feature, feat)\n",
    "\n",
    "        # Envelope RMS\n",
    "        smoothed = rms(signal)\n",
    "        smoothed = smoothed.reshape(-1,)\n",
    "        smoothed /= np.max(np.abs(smoothed))\n",
    "        # varnz\n",
    "        feat = np.var(smoothed)\n",
    "        #feature = np.append(feature, feat)\n",
    "        #std\n",
    "        feat = np.std(smoothed)/np.mean(smoothed)\n",
    "        feature = np.append(feature, feat)\n",
    "        #momentum\n",
    "        t = time_vector(smoothed, duration)\n",
    "        feat = np.dot(smoothed, t)/np.sum(smoothed)\n",
    "        feature = np.append(feature, feat)\n",
    "\n",
    "        #ZCR\n",
    "        filtered = band_pass_filter(signal, sr, cuton, cutoff)\n",
    "        zcr = librosa.feature.zero_crossing_rate(filtered, frame_length=FRAME_SIZE, hop_length=HOP_SIZE)[0]\n",
    "        zcr /= np.max(np.abs(zcr))\n",
    "        #mean\n",
    "        feat = np.mean(zcr)\n",
    "        feature = np.append(feature, feat)\n",
    "        #maximum\n",
    "        feat = np.max(zcr)\n",
    "        #feature = np.append(feature, feat)\n",
    "        #varnz\n",
    "        feat = np.var(zcr)\n",
    "        #feature = np.append(feature, feat)\n",
    "        #std\n",
    "        feat = np.std(zcr)/np.mean(zcr)\n",
    "        feature = np.append(feature, feat)\n",
    "\n",
    "        #MFCCS\n",
    "        mfccs = librosa.feature.mfcc(y = signal, sr=sr, n_mfcc = n_mfcc, n_fft = FRAME_SIZE, hop_length = HOP_SIZE)\n",
    "        #mean\n",
    "        feat = np.mean(mfccs[:, ((mfccs.shape[1]*2)//5 - 5) : ((mfccs.shape[1]*2)//5 + 5)], axis = 1)\n",
    "        feat = feat[1]\n",
    "        feature = np.append(feature, feat)\n",
    "        #maximum\n",
    "        feat = np.max(mfccs[:, ((mfccs.shape[1]*2)//5 - 5) : ((mfccs.shape[1]*2)//5 + 5)], axis = 1)\n",
    "        feat = feat[3]\n",
    "        feature = np.append(feature, feat)\n",
    "        \n",
    "        mfccs /= np.max(np.abs(mfccs), axis = 1, keepdims=True)\n",
    "\n",
    "        #vrnz\n",
    "        feat = np.var(mfccs, axis = 1)\n",
    "        feat = feat[1]\n",
    "        #feature = np.append(feature, feat)\n",
    "        #std\n",
    "        feat = np.std(mfccs, axis = 1)/np.mean(mfccs, axis = 1)\n",
    "        feat = feat[1]\n",
    "        feature = np.append(feature, feat)\n",
    "        #momentum\n",
    "        frames = range(mfccs.shape[1])\n",
    "        t = librosa.frames_to_time(frames, sr=sr, n_fft = FRAME_SIZE, hop_length = HOP_SIZE)\n",
    "        feat = np.dot(mfccs, t)/np.sum(mfccs, axis = 1)\n",
    "        feat = feat[0]\n",
    "        feature = np.append(feature, feat)\n",
    "\n",
    "        #hilbert envelope\n",
    "        env = smooth_envelope(signal, sr, 45)\n",
    "        selected = np.linspace(0, len(env) - 1, 30, dtype=int)\n",
    "        env = env[selected]\n",
    "        env = env.reshape(-1,1)\n",
    "        feat = env[11]\n",
    "        feature = np.append(feature, feat)\n",
    "        feat = env[12]\n",
    "        feature = np.append(feature, feat)\n",
    "\n",
    "        if features[fruit] is not None:\n",
    "            features[fruit] = np.vstack([features[fruit], feature])\n",
    "        else:\n",
    "            features[fruit] = feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA\n",
    "whole = np.concatenate(list(features.values()), axis=0)\n",
    "\n",
    "#Paso 2: Aplicar PCA para obtener dos componentes principales\n",
    "pca = PCA(n_components = 3)\n",
    "reduced_features = pca.fit_transform(whole)\n",
    "\n",
    "#Paso 3: Crear un diccionario con las matrices reducidas\n",
    "reduced = {}\n",
    "start_idx = 0\n",
    "\n",
    "for fruit, matrix in features.items():\n",
    "    num_rows = matrix.shape[0]\n",
    "    reduced[fruit] = reduced_features[start_idx:start_idx + num_rows, :]\n",
    "    start_idx += num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_features3d(reduced)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
