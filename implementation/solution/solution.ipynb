{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; vertical-align: middle;\">\n",
    "\n",
    "# IMAGEN\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LIBRERIAS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pylab as plt\n",
    "from sklearn.cluster import KMeans\n",
    "import joblib\n",
    "from scipy.spatial.distance import cdist\n",
    "import math\n",
    "import librosa\n",
    "import librosa.display\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sounddevice as sd\n",
    "from scipy.signal import butter, lfilter\n",
    "import soundfile as sf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SHELFS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path      = '../../dataset/images'\n",
    "shelfs_path     = os.path.join(image_path, 'test')\n",
    "shelf_names     = ['shelf1', 'shelf2', 'shelf3', 'shelf4']\n",
    "training_data   = '../images/kmeans/training_data.pkl'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DICCIONARIO DE IMAGENES RAW**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "shelf_files = dict()\n",
    "for shelf in shelf_names:\n",
    "    shelf_dir      = os.path.join(shelfs_path, shelf)\n",
    "    shelf_original = os.path.join(shelf_dir, 'original')\n",
    "    image_files    = os.listdir(shelf_original)\n",
    "\n",
    "    shelf_files[shelf] = os.path.join(shelf_original, image_files[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PROCESAMIENTO DE LAS IMÁGENES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_light_background(mask, f = 20, p = 0.75):\n",
    "    height, width = mask.shape\n",
    "    cluster_size  = min([height, width])//f\n",
    "    cluster       = np.ones((cluster_size, cluster_size), np.uint8)\n",
    "\n",
    "    # Corners\n",
    "    corner1 = np.bitwise_and(cluster, mask[:cluster_size,  :cluster_size])\n",
    "    corner2 = np.bitwise_and(cluster, mask[:cluster_size:, -cluster_size:])\n",
    "    corner3 = np.bitwise_and(cluster, mask[-cluster_size:, :cluster_size])\n",
    "    corner4 = np.bitwise_and(cluster, mask[-cluster_size:, -cluster_size:])\n",
    "    corners = [corner1, corner2, corner3, corner4]\n",
    "\n",
    "    # Sides\n",
    "    limitw1 = (width - cluster_size)//2\n",
    "    limitw2 = (width + cluster_size)//2\n",
    "    limith1 = (height - cluster_size)//2\n",
    "    limith2 = (height + cluster_size)//2\n",
    "    \n",
    "    side1   = np.bitwise_and(cluster, mask[:cluster_size, limitw1:limitw2])\n",
    "    side2   = np.bitwise_and(cluster, mask[limith1:limith2, :cluster_size])\n",
    "    side3   = np.bitwise_and(cluster, mask[limith1:limith2, -cluster_size:])\n",
    "    side4   = np.bitwise_and(cluster, mask[-cluster_size:, limitw1:limitw2])\n",
    "    sides   = [side1, side2, side3, side4] \n",
    "    \n",
    "    # Determining the type of background\n",
    "    edges            = corners + sides\n",
    "    light_background = sum(np.count_nonzero(edge) for edge in edges) > p*8*(cluster_size**2)\n",
    "\n",
    "    # Inverting if dark background\n",
    "    if light_background:\n",
    "        return np.bitwise_not(mask)\n",
    "    return mask    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "for shelf, file in shelf_files.items():\n",
    "    # BGR image\n",
    "    image = cv2.imread(file)\n",
    "\n",
    "    # Dimenssions\n",
    "    height, width, _ = image.shape\n",
    "    \n",
    "    # Pixel data vector\n",
    "    data_vector = np.zeros((height * width, 4))\n",
    "\n",
    "    # Obtener matrices del espacio de colores\n",
    "    rgb_matrix = image.reshape((-1, 3))\n",
    "    hsv_matrix = cv2.cvtColor(image, cv2.COLOR_BGR2HSV).reshape((-1, 3))\n",
    "    lab_matrix = cv2.cvtColor(image, cv2.COLOR_BGR2LAB).reshape((-1, 3))\n",
    "\n",
    "    # Asignar a la matriz de datos\n",
    "    # Conservamos el canal G, S, A y B\n",
    "    data_vector[:, 0]  = rgb_matrix[:, 2]\n",
    "    data_vector[:, 1]  = hsv_matrix[:, 1]\n",
    "    data_vector[:, 2:] = lab_matrix[:, 1:]\n",
    "\n",
    "    # Segmentamos la imagen con los vectores obtenidos pos cada pixel\n",
    "    kmeans = KMeans(n_clusters = 2, n_init = 10)  # 2 Clusters. Background and fruit\n",
    "    kmeans.fit(data_vector)\n",
    "\n",
    "    # Get clusters labels\n",
    "    labels = kmeans.labels_\n",
    "\n",
    "    # kmeans_mask\n",
    "    kmeans_mask = labels.reshape(height, width)\n",
    "    kmeans_mask = kmeans_mask.astype(np.uint8) * 255\n",
    "\n",
    "    # Determinación del tipo de fondo de la máscara\n",
    "    kmeans_mask = get_light_background(kmeans_mask)\n",
    "\n",
    "    # Erosion y dilataciòn sobre la màscara\n",
    "    erosion_size      = min([height, width])//200\n",
    "    dilatacion_size   = min([height, width])//80\n",
    "    kernel_erosion    = np.ones((erosion_size,erosion_size), np.uint8)\n",
    "    eroded            = cv2.erode(kmeans_mask, kernel_erosion, iterations = 1)\n",
    "    kernel_dilatacion = np.ones((dilatacion_size,dilatacion_size), np.uint8)\n",
    "    kmeans_mask       = cv2.dilate(eroded, kernel_dilatacion, iterations  = 2)\n",
    "\n",
    "    # Encontrar contornos\n",
    "    kmeans_cnt, _ = cv2.findContours(kmeans_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    kmeans_cnt    = max(kmeans_cnt, key = cv2.contourArea)\n",
    "\n",
    "    # Contorno aproximado\n",
    "    epsilon       = 0.001 * cv2.arcLength(kmeans_cnt, True)\n",
    "    kmeans_cnt    = cv2.approxPolyDP(kmeans_cnt, epsilon, True)\n",
    "    kmeans_cnt    = (kmeans_cnt,)\n",
    "\n",
    "    # Template\n",
    "    tkmeans       = np.zeros((height, width), dtype=np.uint8)\n",
    "\n",
    "    # Dibujar\n",
    "    cv2.drawContours(tkmeans, kmeans_cnt, -1, 255, thickness = cv2.FILLED)\n",
    "\n",
    "    # Guardar mascara\n",
    "    cv2.imwrite(os.path.join(shelfs_path, f\"{shelf}/processed/{os.path.basename(file)}\"), tkmeans)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DICCIONARIO DE MÁSCARAS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "shelf_masks = dict()\n",
    "for shelf in shelf_names:\n",
    "    shelf_dir     = os.path.join(shelfs_path, shelf)\n",
    "    shelf_mask    = os.path.join(shelf_dir, 'processed')\n",
    "    mask_files    = os.listdir(shelf_mask)\n",
    "\n",
    "    shelf_masks[shelf] = os.path.join(shelf_mask, mask_files[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RANGOS DE COLOR**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_red_2 = np.array([170, 60, 60])\n",
    "upper_red_2 = np.array([179, 255, 255])\n",
    "\n",
    "lower_red_1 = np.array([0, 60, 60])\n",
    "upper_red_1 = np.array([8, 255, 255])\n",
    "\n",
    "lower_orange = np.array([8, 120, 80])\n",
    "upper_orange = np.array([21, 255, 255])\n",
    "\n",
    "lower_yellow = np.array([21, 50, 80])\n",
    "upper_yellow = np.array([25, 255, 255])\n",
    "\n",
    "lower_green = np.array([25, 40, 40])\n",
    "upper_green = np.array([100, 255, 255])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXTRACCIÓN DE CARACTERÍSTICAS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversion_color = {'V' :-20, 'R' : -10, 'A' : 10, 'N' : 20}\n",
    "image_features   = dict.fromkeys(shelf_names)\n",
    "\n",
    "for shelf, image_file, mask_file in zip(shelf_files.keys(), shelf_files.values(), shelf_masks.values()):\n",
    "    # Leer la imagen y la máscara\n",
    "    image = cv2.imread(image_file)\n",
    "    mask  = cv2.imread(mask_file, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    # Convertir la imagen de BGR a HSV\n",
    "    hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    # Aplicar la máscara\n",
    "    fruit = cv2.bitwise_and(hsv_image, hsv_image, mask=mask)\n",
    "\n",
    "    #---------------Extracción de los momentos de Hu----------------------\n",
    "\n",
    "    # Encontrar el rectángulo delimitador de la fruta\n",
    "    (x, y, w, h) = cv2.boundingRect(mask)\n",
    "\n",
    "    # Recortar la imagen original para obtener solo la región de la fruta\n",
    "    trimed  = fruit[y:y + h, x:x + w]\n",
    "\n",
    "    # Convertir la imagen a escala de grises si es necesario\n",
    "    trimed_gray = cv2.cvtColor(trimed, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Calcular los momentos de la imagen\n",
    "    momentos = cv2.moments(trimed_gray)\n",
    "\n",
    "    # Calcular los momentos de Hu\n",
    "    momentos_hu = cv2.HuMoments(momentos)\n",
    "\n",
    "    # Aplicar logaritmo a los momentos de Hu para mejorar la escala\n",
    "    log_moments_hu = -np.sign(momentos_hu) * np.log10(np.abs(momentos_hu))\n",
    "    moments = log_moments_hu.reshape(-1)\n",
    "\n",
    "    #-----------------------Extracción de color-------------------------\n",
    "    conteo = {\n",
    "        'V' : np.sum(np.all(np.logical_and(lower_green  <= fruit, fruit <= upper_green), axis=-1)),\n",
    "        'R1': np.sum(np.all(np.logical_and(lower_red_1  <= fruit, fruit <= upper_red_1), axis=-1)),\n",
    "        'R2': np.sum(np.all(np.logical_and(lower_red_2  <= fruit, fruit <= upper_red_2), axis=-1)),\n",
    "        'A' : np.sum(np.all(np.logical_and(lower_yellow <= fruit, fruit <= upper_yellow), axis=-1)),\n",
    "        'N' : np.sum(np.all(np.logical_and(lower_orange <= fruit, fruit <= upper_orange), axis=-1))\n",
    "    }\n",
    "    conteo_por_rango = {\n",
    "        'V': conteo['V'],\n",
    "        'R': conteo['R1'] + conteo['R2'],\n",
    "        'A': conteo['A'],\n",
    "        'N': conteo['N']\n",
    "    }\n",
    "\n",
    "    sorted_conteo = sorted(conteo_por_rango.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Obtener el segundo elemento más grande\n",
    "    segundo_mas_grande = sorted_conteo[1]\n",
    "\n",
    "    # Obtener la etiqueta y el valor del segundo elemento más grande\n",
    "    etiqueta_segundo_mas_grande = segundo_mas_grande[0]\n",
    "    valor_segundo_mas_grande    = segundo_mas_grande[1]\n",
    "\n",
    "    # Obtener la etiqueta basándose en el rango con el mayor conteo\n",
    "    etiqueta = max(conteo_por_rango, key = conteo_por_rango.get)\n",
    "\n",
    "    # Se usa el hecho de que a excepción de las manzanas, el resto de las frutas tienen poco rojo\n",
    "    if (etiqueta_segundo_mas_grande == 'R')and(valor_segundo_mas_grande > 0.35*conteo_por_rango[etiqueta]):\n",
    "        etiqueta = 'R'\n",
    "\n",
    "    color = conversion_color[etiqueta]\n",
    "\n",
    "    #-----------------Vector de características----------------------\n",
    "    image_features[shelf] = np.append(moments[2:4], color)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RECUPERAMOS LOS CENTROIDES Y APLICACIÓN DE KNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn(training, test, k_n):\n",
    "    X           = np.concatenate([v for v in training.values()], axis = 0)\n",
    "    y           = np.concatenate([[k] * v.shape[0] for k, v in training.items()])\n",
    "    dist        = cdist(test, X)\n",
    "    sorted_ind  = np.argsort(dist, axis = 1)\n",
    "    sorted_k    = sorted_ind[:, 0:k_n]\n",
    "    predicted   = []\n",
    "    \n",
    "    for row in sorted_k:\n",
    "        labels     = list(y[row])\n",
    "        prediction = max(set(labels), key = labels.count)\n",
    "        predicted.append(prediction)\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "data              = joblib.load(training_data)\n",
    "centroids         = data['centroids']\n",
    "prediction        = knn(centroids, np.vstack(list(image_features.values())), 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CLASIFICACIÓN DE LAS FRUTAS EN LOS ESTANTES**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shelfs = dict(zip(image_features.keys(), prediction))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; vertical-align: middle;\">\n",
    "\n",
    "# AUDIO\n",
    "\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RUTAS, TIPOS DE FRUTAS Y MODELO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fruit_types      = ['pera', 'banana', 'manzana', 'naranja']\n",
    "dataset_path     = '../../dataset/audios/test'\n",
    "original_path    = os.path.join(dataset_path, 'original')\n",
    "processed_path   = os.path.join(dataset_path, 'processed')\n",
    "model_file       = '../audio/knn/model.pkl'\n",
    "model            = dict.fromkeys(['pca', 'features', 'scaler'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PARAMETROS DEL AUDIO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FRAME_SIZE = 512# In the documentation says it's convenient for speech.C\n",
    "HOP_SIZE   = int(FRAME_SIZE/2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FUNCIONES GENERALES DE AUDIO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio(audiofile):\n",
    "    test_audio, sr = librosa.load(audiofile, sr = None)\n",
    "    duration = librosa.get_duration(filename=audiofile, sr=sr)\n",
    "    return test_audio, sr, duration"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FILTERS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def band_pass_filter(signal, sr, low_cutoff, high_cutoff):\n",
    "    b, a = butter(N=3, Wn = [low_cutoff, high_cutoff], btype='band', fs=sr)\n",
    "    return lfilter(b, a, signal)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PROCCESSING OF THE AUDIO FILES FUNCTIONS**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Processing*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectral_flux(signal):\n",
    "\n",
    "    # Calcular el espectrograma de magnitudes\n",
    "    spectrogram = np.abs(librosa.stft(signal, n_fft = FRAME_SIZE, hop_length = HOP_SIZE))\n",
    "\n",
    "    # Calcular el flujo espectral\n",
    "    spectral_flux_values = np.sum(np.diff(spectrogram, axis=1)**2, axis=0)\n",
    "\n",
    "    return spectral_flux_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(audio_in, audio_out, rms_umbral = 0.043, flux_umbral = 0.096):\n",
    "    signal, sr, _ = load_audio(audio_in)\n",
    "\n",
    "    rms = librosa.feature.rms(y = signal, frame_length = FRAME_SIZE, hop_length = HOP_SIZE)\n",
    "    rms /= np.max(np.abs(rms))\n",
    "    trms = librosa.times_like(rms, sr = sr, hop_length = HOP_SIZE, n_fft = FRAME_SIZE)\n",
    "    trms /= trms[-1]\n",
    "\n",
    "    flux = spectral_flux(signal)\n",
    "    flux /= np.max(np.abs(flux))\n",
    "    fluxframes = range(len(flux))\n",
    "    tflux = librosa.frames_to_time(fluxframes, hop_length=HOP_SIZE, n_fft = FRAME_SIZE)\n",
    "    tflux /= tflux[-1]\n",
    "                \n",
    "    left_index = np.argmax(np.abs(flux) > flux_umbral)\n",
    "    rigth_index = len(flux) - 1 - np.argmax(np.abs(np.flip(flux)) > flux_umbral)\n",
    "\n",
    "    tsignal = librosa.times_like(signal, sr = sr, hop_length=HOP_SIZE, n_fft=FRAME_SIZE)\n",
    "    tsignal /= tsignal[-1]\n",
    "\n",
    "    flag      = False\n",
    "    pad_left  = 0\n",
    "    pad_rigth = 0\n",
    "    flag_left  =  False\n",
    "    flag_rigth =  False\n",
    "                \n",
    "    while not flag:\n",
    "        if rms[0, left_index] > rms_umbral:\n",
    "            if left_index > pad_left + 15:\n",
    "                rms_left = left_index - np.argmax(np.flip(np.abs(rms[0, :left_index]) < rms_umbral))\n",
    "                if rms_left <= 0:\n",
    "                    rms_left = left_index\n",
    "                flag_left = True\n",
    "            else:\n",
    "                pad_left += 15\n",
    "                left_index = pad_left + np.argmax(np.abs(flux[pad_left:]) > flux_umbral)\n",
    "        else:\n",
    "                rms_left = left_index\n",
    "                flag_left = True\n",
    "\n",
    "        if rms[0, rigth_index] > rms_umbral:\n",
    "            if rigth_index < (len(flux) - 1 - pad_rigth-15):\n",
    "                rms_rigth = rigth_index + np.argmax(np.abs(rms[0, rigth_index:]) < rms_umbral)\n",
    "                if rms_rigth >= len(flux):\n",
    "                    rms_rigth = rigth_index\n",
    "                flag_rigth = True\n",
    "            else:\n",
    "                pad_rigth += 15\n",
    "                rigth_index = len(flux[:-pad_rigth]) - 1 - np.argmax(np.flip(np.abs(flux[:-pad_rigth]) > flux_umbral))                               \n",
    "        else:\n",
    "            rms_rigth = rigth_index\n",
    "            flag_rigth = True\n",
    "\n",
    "        flag = flag_left and flag_rigth\n",
    "\n",
    "    left_index  = min(left_index, rms_left)\n",
    "    rigth_index = max(rigth_index, rms_rigth)\n",
    "    mask        = tsignal >= tflux[left_index]\n",
    "    ttrimed     = tsignal[mask]\n",
    "    trimed      = signal[mask]\n",
    "    mask        = ttrimed <= tflux[rigth_index]\n",
    "    ttrimed     = ttrimed[mask]\n",
    "    trimed      = trimed[mask]\n",
    "    \n",
    "    sf.write(audio_out, trimed, sr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FEATURES EXTRACTION**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Features extraction functions*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_split_frequency_bin(split_frequency, sample_rate, num_frequency_bins):\n",
    "    \"\"\"Infer the frequency bin associated to a given split frequency.\"\"\"\n",
    "    \n",
    "    frequency_range = sample_rate / 2\n",
    "    frequency_delta_per_bin = frequency_range / num_frequency_bins\n",
    "    split_frequency_bin = math.floor(split_frequency / frequency_delta_per_bin)\n",
    "    return int(split_frequency_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def band_energy_ratio(spectrogram, split_frequency, sample_rate):\n",
    "    \"\"\"Calculate band energy ratio with a given split frequency.\"\"\"\n",
    "    \n",
    "    split_frequency_bin = calculate_split_frequency_bin(split_frequency, sample_rate, len(spectrogram[0]))\n",
    "    band_energy_ratio = []\n",
    "    \n",
    "    # calculate power spectrogram\n",
    "    power_spectrogram = np.abs(spectrogram) ** 2\n",
    "    power_spectrogram = power_spectrogram.T\n",
    "    \n",
    "    # calculate BER value for each frame\n",
    "    for frame in power_spectrogram:\n",
    "        sum_power_low_frequencies = frame[:split_frequency_bin].sum()\n",
    "        sum_power_high_frequencies = frame[split_frequency_bin:].sum()\n",
    "        band_energy_ratio_current_frame = sum_power_low_frequencies / (sum_power_high_frequencies + sum_power_low_frequencies)\n",
    "        band_energy_ratio.append(band_energy_ratio_current_frame)\n",
    "    \n",
    "    return np.array(band_energy_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rms(signal, frames, hop):\n",
    "    return librosa.feature.rms(y=signal, frame_length = frames, hop_length = hop)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Function to get the features*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(signal, sr):\n",
    "    feature = np.empty((1, 0))\n",
    "\n",
    "    # BER\n",
    "    spec = librosa.stft(signal, n_fft = FRAME_SIZE, hop_length = HOP_SIZE)\n",
    "    # max\n",
    "    split_frequency = 600\n",
    "    BER  = band_energy_ratio(spec, split_frequency, sr)\n",
    "    feat = np.max(np.abs(BER))\n",
    "    feature = np.append(feature, feat)\n",
    "    # min\n",
    "    # 1\n",
    "    split_frequency = 1900\n",
    "    BER  = band_energy_ratio(spec, split_frequency, sr)\n",
    "    feat = np.min(np.abs(BER))\n",
    "    feature = np.append(feature, feat)\n",
    "    # 2\n",
    "    split_frequency = 5000\n",
    "    BER  = band_energy_ratio(spec, split_frequency, sr)\n",
    "    feat = np.min(np.abs(BER))\n",
    "    feature = np.append(feature, feat)\n",
    "    # 3\n",
    "    split_frequency = 9000\n",
    "    BER  = band_energy_ratio(spec, split_frequency, sr)\n",
    "    feat = np.min(np.abs(BER))\n",
    "    feature = np.append(feature, feat)\n",
    "    # std\n",
    "    # 1\n",
    "    split_frequency = 8000\n",
    "    BER  = band_energy_ratio(spec, split_frequency, sr)\n",
    "    BER /= np.max(np.abs(BER))\n",
    "    feat = np.std(BER)/np.mean(np.abs(BER))\n",
    "    feature = np.append(feature, feat)\n",
    "    # 2\n",
    "    split_frequency = 1000\n",
    "    BER  = band_energy_ratio(spec, split_frequency, sr)\n",
    "    BER /= np.max(np.abs(BER))\n",
    "    feat = np.std(BER)/np.mean(np.abs(BER))\n",
    "    feature = np.append(feature, feat)\n",
    "\n",
    "    #ZCR\n",
    "    cutoff = 5000\n",
    "    cuton = 1000\n",
    "    filtered = band_pass_filter(signal, sr, cuton, cutoff)\n",
    "    zcr = librosa.feature.zero_crossing_rate(filtered, frame_length=FRAME_SIZE, hop_length=HOP_SIZE)[0]\n",
    "    zcr /= np.max(np.abs(zcr))\n",
    "    # mean\n",
    "    feat = np.mean(zcr)\n",
    "    feature = np.append(feature, feat)\n",
    "    # maximum\n",
    "    cutoff = 10000\n",
    "    cuton = 10\n",
    "    filtered = band_pass_filter(signal, sr, cuton, cutoff)\n",
    "    zcr = librosa.feature.zero_crossing_rate(filtered, frame_length=FRAME_SIZE, hop_length=HOP_SIZE)[0]\n",
    "    feat = np.max(np.abs(zcr))\n",
    "    feature = np.append(feature, feat)\n",
    "    # std\n",
    "    cutoff = 10000\n",
    "    cuton  = 20\n",
    "    filtered = band_pass_filter(signal, sr, cuton, cutoff)\n",
    "    zcr = librosa.feature.zero_crossing_rate(filtered, frame_length=FRAME_SIZE, hop_length=HOP_SIZE)[0]\n",
    "    feat = np.std(zcr)/np.mean(np.abs(zcr))\n",
    "    feature = np.append(feature, feat)\n",
    "    # mean local\n",
    "    cutoff = 5000\n",
    "    cuton = 1000\n",
    "    filtered = band_pass_filter(signal, sr, cuton, cutoff)\n",
    "    zcr = librosa.feature.zero_crossing_rate(filtered, frame_length=FRAME_SIZE, hop_length=HOP_SIZE)[0]\n",
    "    zcr /= np.max(np.abs(zcr))\n",
    "    feat = np.mean(zcr[((len(zcr)*3)//14 - 5) : ((len(zcr)*3)//14 + 5)])\n",
    "    feature = np.append(feature, feat)\n",
    "    # local max\n",
    "    cutoff = 10000\n",
    "    cuton = 10\n",
    "    filtered = band_pass_filter(signal, sr, cuton, cutoff)\n",
    "    zcr = librosa.feature.zero_crossing_rate(filtered, frame_length=FRAME_SIZE, hop_length=HOP_SIZE)[0]\n",
    "    feat = np.max(zcr[((len(zcr)*3)//4 - 10) : ((len(zcr)*3)//4 + 10)])\n",
    "    feature = np.append(feature, feat)\n",
    "\n",
    "    # Roll off\n",
    "    cuton = 100\n",
    "    cutoff = 8500\n",
    "    filtered = band_pass_filter(signal, sr, cuton, cutoff)\n",
    "    roll_off = librosa.feature.spectral_rolloff(y=filtered, sr=sr, n_fft=FRAME_SIZE, hop_length=HOP_SIZE, roll_percent=0.28)[0]\n",
    "    roll_off /= np.max(np.abs(roll_off))\n",
    "    # mean\n",
    "    feat = np.mean(np.abs(roll_off))\n",
    "    feature = np.append(feature, feat)\n",
    "    # max\n",
    "    cuton = 100\n",
    "    cutoff = 8500\n",
    "    filtered = band_pass_filter(signal, sr, cuton, cutoff)\n",
    "    roll_off = librosa.feature.spectral_rolloff(y=filtered, sr=sr, n_fft=FRAME_SIZE, hop_length=HOP_SIZE, roll_percent=0.55)[0]\n",
    "    feat = np.max(np.abs(roll_off))\n",
    "    feature = np.append(feature, feat)\n",
    "    # std\n",
    "    cutoff = 8500\n",
    "    cuton = 50\n",
    "    filtered = band_pass_filter(signal, sr, cuton, cutoff)\n",
    "    roll_off = librosa.feature.spectral_rolloff(y=filtered, sr=sr, n_fft=FRAME_SIZE, hop_length=HOP_SIZE, roll_percent=0.28)[0]\n",
    "    roll_off /= np.max(np.abs(roll_off))\n",
    "    feat = np.std(np.abs(roll_off))/np.mean(np.abs(roll_off))\n",
    "    feature = np.append(feature, feat)\n",
    "\n",
    "    #MFCCS\n",
    "    n_mfcc = 4\n",
    "    # 1\n",
    "    cuton = 500\n",
    "    cutoff = 5000\n",
    "    filtered = band_pass_filter(signal, sr, cuton, cutoff)\n",
    "    mfccs = librosa.feature.mfcc(y = signal, sr=sr, n_mfcc = n_mfcc, n_fft = FRAME_SIZE, hop_length = HOP_SIZE)\n",
    "    feat = np.max(mfccs, axis = 1)\n",
    "    feature = np.append(feature, feat[3])\n",
    "    \n",
    "    # 2\n",
    "    cuton = 10\n",
    "    cutoff = 8000\n",
    "    filtered = band_pass_filter(signal, sr, cuton, cutoff)\n",
    "    mfccs = librosa.feature.mfcc(y = filtered, sr=sr, n_mfcc = n_mfcc, n_fft = FRAME_SIZE, hop_length = HOP_SIZE)\n",
    "    mfccs /= np.max(np.abs(mfccs), axis = 1, keepdims=True)\n",
    "    feat = np.std(np.abs(mfccs), axis = 1)/np.mean(np.abs(mfccs), axis = 1)\n",
    "    feature = np.append(feature, feat[3])\n",
    "\n",
    "    # 3\n",
    "    cuton = 10\n",
    "    cutoff = 8000\n",
    "    filtered = band_pass_filter(signal, sr, cuton, cutoff)\n",
    "    mfccs = librosa.feature.mfcc(y = filtered, sr=sr, n_mfcc = n_mfcc, n_fft = FRAME_SIZE, hop_length = HOP_SIZE)\n",
    "    mfccs /= np.max(np.abs(mfccs), axis = 1, keepdims=True)\n",
    "    mfccs = mfccs[:, ((mfccs.shape[1]*4) // 5 - 10):((mfccs.shape[1]*4) //5 + 10)]\n",
    "    feat = np.std(np.abs(mfccs), axis=1) / np.mean(np.abs(mfccs), axis=1)\n",
    "    feature = np.append(feature, feat[1])\n",
    "\n",
    "    #envelope\n",
    "    env = rms(signal, FRAME_SIZE, HOP_SIZE)\n",
    "    env = env.reshape(-1,)\n",
    "    selected = np.linspace(0, len(env) - 1, 30, dtype=int)\n",
    "    env = env[selected]\n",
    "    feat = env[11]\n",
    "    feature = np.append(feature, feat)\n",
    "    feat = env[12]\n",
    "    feature = np.append(feature, feat)\n",
    "\n",
    "    return feature"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AUDIO RECORGING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration = 2.5      # Duración de la grabación en segundos\n",
    "fs       = 48000    # Frecuencia de muestreo en Hz\n",
    "\n",
    "input('Presione ENTER cuando este listo para grabar')\n",
    "print(\"Grabando...\")\n",
    "\n",
    "data = sd.rec(int(duration * fs), samplerate = fs, channels = 1, dtype = 'int16')\n",
    "sd.wait()\n",
    "print(\"Grabación completa.\")\n",
    "\n",
    "orden = os.path.join(original_path, f\"{len(os.listdir(original_path)) + 1}.wav\")\n",
    "sf.write(orden, data, fs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PROCESSING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_order = os.path.join(processed_path, f\"test{len(os.listdir(processed_path)) + 1}.wav\")\n",
    "process(orden, processed_order)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FEATURE EXTRACTION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal, sr, _ = load_audio(processed_order)\n",
    "feature       = get_features(signal, sr)\n",
    "test_features = feature.reshape(1, -1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LOAD THE REDUCED MODEL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model        = joblib.load(model_file)\n",
    "reduced:dict = model['features']\n",
    "pca          = model['pca']\n",
    "scaler       = model['scaler']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TRANSFORM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_test_features   = scaler.transform(test_features)\n",
    "reduced_test           = pca.transform(scaled_test_features)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PREDICTION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction  = knn(reduced, reduced_test, 3)\n",
    "place = [shelf for shelf, fruit in shelfs.items() if fruit == prediction]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RESULTADO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(place) == 0:\n",
    "    print(f\"No se encontró una {prediction} en ninguna de las 4 estanterías.\")\n",
    "elif len(place) == 1:\n",
    "    print(f\"Se encontró una {prediction} en la {place[0]}\")\n",
    "elif len(place) > 1:\n",
    "    print(f\"Se encontró una {prediction} en las siguientes estanterias: {place}\")\n",
    "\n",
    "fig, axs = plt.subplots(1, 4, figsize = (15, 5))\n",
    "i = 0\n",
    "for shelf, file in shelf_files.items():\n",
    "    i += 1\n",
    "    image = cv2.imread(file)\n",
    "    axs[i].imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    axs[i].axis('off')\n",
    "    axs[i].set_title(shelf)\n",
    "\n",
    "    if shelf in place:\n",
    "        h, w, _ = image.shape\n",
    "        rect = plt.Rectangle((0, 0), w, h, linewidth = 2, edgecolor = 'green', facecolor = 'none')\n",
    "        axs[i].add_patch(rect)\n",
    "\n",
    "plt.subplots_adjust(wspace = 0.5)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
